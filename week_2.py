# -*- coding: utf-8 -*-
"""2nd assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13HoHo8Kfv6RiAg28FGJ0Z_Op_G5cnBGs

**Question 1** : In the folowing definition of a probabilistic model:

$Y \sim \mathcal{N}(\mu, \sigma)$

$\mu \sim \mathcal{N}(0, 2)$

$\sigma \sim \mathcal{halfnormal}(0.75)$

1.1 - Identify the prior and the likelihood.

1.2  - How many parameters will the posterior have?

1.3 - Compare it with the model for the coin-flipping problem.

1.4 - Write Bayes' theorem for the model.

**(1.1)**
- Priors Distributions are :

\begin{align*}
\mu &\sim N(0, 2) \\
\sigma &\sim \text{halfnormal}(0.75)
\end{align*}
- Likelihood: The likelihood function represents the probability of the data \(Y\) given the parameters (’μ’ and ‘σ’). So, the normal distribution:

\begin{align*}
Y &\sim N(\mu, \sigma) \\
\end{align*}
**(1.2)**

- In this model posterior have 2 parameters:
 ***’μ’*** and ***‘σ’***.

**(1.3)**

- The coin-flipping problem typically involves a Binomial likelihood and a Beta prior. Specifically, if \(Y\) is the number of heads in \(n\) flips:

\begin{align*}
\text{Likelihood}: & \quad Y \mid p \sim \text{Binomial}(n, p) \\
\text{Prior}: & \quad p \sim \text{Beta}(\alpha, \beta)
\end{align*}

Here, there is only **one** parameter \(p\) which represents the probability of getting head by flipping the coin once.

Comparing the two models:

1. Dependencies over Prior and Posterior -

- This normal model involves **two continuous parameters** ’μ’ and ‘σ’.
- The coin-flipping model involves only **one continuous parameter** ‘p’.
2. Nature of distribution for Priors and Likelihood -

- This model has one prior as Normally distributed and one prior as Half-Normal. Whereas, the likelihood is also a Normal distribution.
- The coin-flipping problem typically involves a Beta-Binomial likelihood and a Beta prior.

3. Complexity of the Posterior Distribution -

- The posterior distribution will be a joint distribution over two parameters, which increases the complexity. The computation of the posterior may become more complex. Visualization is also more challenging as it involves plotting in two dimensions.

  Posterior Example:
  
$$
P(\mu, \sigma \mid Y) = \frac{P(Y \mid \mu, \sigma) P(\mu) P(\sigma)}{P(Y)}
$$

- Since, it is a function of a single parameter. This generally makes the analysis and computation simpler. Analytical solutions may be more feasible, and visualization is straightforward since it involves plotting in one dimension.
 Posterior Example:

$$
P(p \mid Y) = \frac{P(Y \mid p) P(p)}{P(Y)}
$$

**(1.4).**

Bayes' theorem for the given model:

$$
P(\mu, \sigma \mid Y) = \frac{P(Y \mid \mu, \sigma) P(\mu) P(\sigma)}{P(Y)}
$$

Substitute the given distributions:

$$
P(\mu, \sigma \mid Y) = \frac{N(Y \mid \mu, \sigma) \cdot N(\mu \mid 0, 2) \cdot \text{halfnormal}(\sigma \mid 0.75)}{P(Y)}
$$

$$
P(\mu, \sigma \mid Y) \propto N(Y \mid \mu, \sigma) \cdot N(\mu \mid 0, 2) \cdot \text{halfnormal}(\sigma \mid 0.75)
$$

**Question 2**

Let’s suppose that we have two coins; when we toss the first coin, half of the time it lands on tails
 and half of the time on heads. The other coin is a loaded coin that always lands on heads. If we take
 one of the coins at random and get a head, what is the probability that this coin is the unfair one?

Let-

- $ H $: Observing a head when a coin is tossed.
- $ E_1 $: Choosing a fair coin.
- $ E_2 $: Choosing an unfair coin.

Probability of getting a head with the fair coin is:$$ P(H \mid E_1) = \frac{1}{2} $$

Probability of getting a head with the unfair coin is:$$ P(H \mid E_2) = 1 $$

Probability of choosing the fair coin is $ p $, and the probability of choosing the unfair coin is $ (1 - p) $:

   $$ P(E_1) = p, \quad P(E_2) = 1 - p $$

We need to find the probability that the coin is the unfair one given that we observed a head:
$$ P(E_2 \mid H) $$

According to Bayes' theorem:

$$ P(E_2 \mid H) = \frac{P(H \mid E_2) P(E_2)}{P(H)} $$

From the law of total probability:

$$ P(H) = P(H \mid E_1)P(E_1) + P(H \mid E_2)P(E_2) $$

As we know,  $$ P(H) = \left( \frac{1}{2} \right) p + 1 \cdot (1 - p) $$


$$ P(H) = 1 - \frac{1}{2} p $$

So, substituting it back into Bayes’ theorem, we get:

$$ P(E_2 \mid H) = \frac{(1) (1 - p)}{1 - \frac{1}{2} p} $$

$$ P(E_2 \mid H) = \frac{1 - p}{1 - \frac{1}{2} p} $$

If the coins are equally likely to be chosen -

Substitute, $ p = \frac{1}{2} $

Therefore, the probability that the coin is the unfair one given that we observed a head is:
$$ P(E_2 \mid H) = \frac{2}{3} $$

**Question 3**


Using PyMC, change the parameters of the prior Beta distribution in `our_first_model` to match those of Week 2. Compare the results.
"""

!pip install pymc==5.8.0 arviz==0.16.1 bambi==0.13.0 pymc-bart==0.5.2 kulprit==0.0.1 preliz==0.3.6 nutpie==0.9.1

# Installing all the requirements.
# !pip install pymc==5.8.0 arviz==0.16.1 bambi==0.13.0 pymc-bart==0.5.2 kulprit==0.0.1 preliz==0.3.6 nutpie==0.9.1

# Importing all the required libraries.
import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pymc as pm
import preliz as pz

# Setting up our_first_model.
az.style.use("arviz-grayscale")
from cycler import cycler
default_cycler = cycler(color=["#000000", "#6a6a6a", "#bebebe", "#2a2eec"])
plt.rc('axes', prop_cycle=default_cycler)
plt.rc('figure', dpi=300)

np.random.seed(123)
trials = 4
theta_real = 0.35 # unknown value in a real experiment
# Defining Likelihood.
data = pz.Binomial(n = 1, p = theta_real).rvs(trials)

# Defining Parameters as per Week-2.

alpha_params = [1, 20, 1]
beta_params = [1, 20, 4]

for i in range(0,3):
  with pm.Model() as our_first_model:
      θ = pm.Beta('θ', alpha=alpha_params[i], beta=beta_params[i])
      y = pm.Bernoulli('y', p=θ, observed=data)
      idata = pm.sample(1000, random_seed=4591)
      print("Parameters : alpha = ", alpha_params[i]," , beta = ", beta_params[i])
      print(az.summary(idata, kind="stats").round(2))
      az.plot_trace(idata)
      az.plot_trace(idata, kind="rank_bars", combined=True, rank_kwargs={"colors": "k"});
      az.plot_posterior(idata, figsize=(12, 4))
      az.plot_bf(idata, var_name="θ", prior=np.random.uniform(0, 1, 10000), ref_val=0.35, figsize=(12, 4), colors=["C0", "C2"])
      az.plot_posterior(idata, rope=[0.25, .45], figsize=(12, 4))
      az.plot_posterior(idata, ref_val=0.35, figsize=(12, 4))

"""Observations :

- Beta($\alpha$ = 1, $\beta$ = 1) is a better parameter to follow under posterior distribution as its mean, Savage-Dickey density ratio (ref = 0.35), Region of Practical Equivalence (ROPE = [.25, .45]) are such that, it is more likely to obtain a $\theta$ = 0.35.

  We can rank the beta-priors in order of better results:

   Beta($\alpha$ = 1, $\beta$ = 1) $>$ Beta($\alpha$ = 1, $\beta$ = 4) $>$ Beta($\alpha$ = 20, $\beta$ = 20)

- In general scenarios, when $\theta_r$ is unknown and we compare our results for $\theta_r$ = 0.5, then the above mentioned order will be as follows-

   Beta($\alpha$ = 20, $\beta$ = 20) $>$ Beta($\alpha$ = 1, $\beta$ = 1) $>$ Beta($\alpha$ = 1, $\beta$ = 4)

- Hence, for stronger prior beliefs we must choose higher values of beta parameters. But they must not be large enough because it may lead to domination of priors in the posterior value obtained. So, small amount of data will not have much impact on the posterior calculation.
"""